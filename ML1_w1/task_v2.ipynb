{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hMIq9t_Fulek",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7364105dce8d63d68941fc8b471bd22",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "This is programming assignment for week 1. In this assignment you will be working with scikit-learn to perform exploratory data analysis and practice working with text data. \n",
    "\n",
    "Please, read all the notebook carefully and make sure that you understand not only the task, but the whole pipeline.\n",
    "\n",
    "### Grading\n",
    "The assignment is automatically graded. \n",
    "\n",
    "**Automatic grading**\n",
    "After you finish solving all the tasks restart the kernel (`kernel -> restart`) and click the button `Validate` to check that everything works as expected. Afterwards, you can submit your work.\n",
    "\n",
    "\n",
    "# Table of Contents:\n",
    "* [Problem 1.](#part1)  Exploratory data analysis\n",
    "     - [Task 1](#task1) [1 pt]\n",
    "     - [Task 2](#task2) [2 pts]\n",
    "     - [Task 3](#task3) [2 pts]\n",
    "     - [Task 4](#task4) [1 pt]\n",
    "     - [Task 5](#task5) [1 pts]\n",
    "     - [Task 6](#task6) [2 pts]\n",
    "     - [Task 7](#task7) [1 pts]\n",
    "   \n",
    "* [Problem 2](#problem2). Text data analysis\n",
    "    - [Task 1](#task2_1) [2 pts]\n",
    "    - [Task 2](#task2_2) [1 pts]\n",
    "    - [Task 3](#task2_3) [1 pts]\n",
    "    - [Task 4](#task2_4) [1 pts]\n",
    "\n",
    "\n",
    "---\n",
    "## Problem 1. Exploratory data analysis <a class=\"anchor\" id=\"part1\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gXOaeCBWOXB5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c046a2445fc77a0301bb0635f1d576",
     "grade": false,
     "grade_id": "cell-88a28302797f3f68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures \n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import  KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GQ9reAhAYSfT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e7f30f9927b94ad1b41285badb34bbc",
     "grade": false,
     "grade_id": "cell-5147a29d38c034d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0vzRNFiurO6X",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbaf4208556b7edcfaad51b7decac2a7",
     "grade": false,
     "grade_id": "cell-421725e4973aa676",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this task we will go through standard exploratory data analysis and preprocessing steps as well as use them to prepare a dataset and train a predictive model.\n",
    "\n",
    "Let's take a look at the data. We will analyze a dataset of diamond characterics, such as size, color and clarity, in order to train a model to predict it's price.\n",
    "\n",
    "<!-- The data used in this task is slighly preprocessed open dataset from kaggle.com https://www.kaggle.com/shivam2503/diamonds  -->\n",
    "\n",
    "First, we need to exclude the target variable, `price`, and split the dataset into train and test sets. We will use the former to perform all the data analysis steps and train the model, and the latter - to assess the model prediction quality on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1930,
     "status": "ok",
     "timestamp": 1607626702573,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "KdlG4LxYVqmT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ca2222755e91ed2186b5b41d5f00593",
     "grade": false,
     "grade_id": "cell-52b3a70201c21899",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e37e6158-c658-4228-a30f-14144bfa79ea"
   },
   "outputs": [],
   "source": [
    "file_path = \"diamonds_prep.csv\"\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GEkiURROPcrB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75a46dda094fc17ccff44ceb5bc5350d",
     "grade": false,
     "grade_id": "cell-82dd190d122fcd4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y = df[\"price\"]\n",
    "X = df.drop(\"price\", axis=1)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OyIYOGktWHsn",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f55e9e113a9c69d63c7be4afad452479",
     "grade": false,
     "grade_id": "cell-5f20df0ca542d0ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This dataset contains both numerical and categorical characteristics. The latter ones take string values, each of which means a certain category. Most standard machine learning algorithms are able to work only with numerical variables, and categorical data have to be additionally prepared before model training. \n",
    "\n",
    "<!-- Also, most of the models usually cannot handle missing values in the data, so we need to check if there are any in our dataset. -->\n",
    "\n",
    "<!-- However, first we will need to check if there are any **missing values** in the data and properly handle them.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1607626704085,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "IRFFmv-5VQpR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b18096b9c36320d15b1386cdc44919d1",
     "grade": false,
     "grade_id": "cell-2d7b5ab032248c4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "99c243de-a35c-4f98-c15c-0feac520f3e6"
   },
   "outputs": [],
   "source": [
    "print(\"Types of variables presented in the training data:\\n\")\n",
    "X_tr.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1938,
     "status": "ok",
     "timestamp": 1607626705181,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "X6y1RzACYP95",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3df32e086c89dd0d0006eff6490b04f3",
     "grade": false,
     "grade_id": "cell-0876d0e36ccb8419",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8bae3eae-5b70-4f4a-d115-72caba1cf767"
   },
   "outputs": [],
   "source": [
    "num_cols = X_tr.columns[(X_tr.dtypes == \"int64\") |(X_tr.dtypes == \"float64\")].tolist() #\n",
    "print(\"Number of numerical features =\", len(num_cols), \"\\n\")\n",
    "\n",
    "print(\"Distribution of numerical features:\")\n",
    "X_tr[num_cols].hist(bins=20, grid=True, figsize=(15, 10), layout=(2, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37ad9a63762f64ea4298e21e8542ec95",
     "grade": false,
     "grade_id": "cell-d4e379677c14714b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values in each column:\\n\")\n",
    "X_tr[num_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1607626707336,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "2E06nhQV3yQe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47f5ac4d7bc0cfec1a096a71ccb9e402",
     "grade": false,
     "grade_id": "cell-66cb8656f2877f5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a8da2406-7127-41ed-c7e0-a62f7bd166d4"
   },
   "outputs": [],
   "source": [
    "cat_cols = X_tr.columns[X_tr.dtypes == \"object\"].tolist() #\n",
    "print(\"Number of categorical features =\", len(cat_cols), \"\\n\")\n",
    "\n",
    "print(\"Distribution of categorical features:\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(cat_cols):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    X_tr[col].value_counts(normalize=True).plot.pie(autopct=lambda x: f\"{round(x, 2)}%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eaf3037f93a59efdc46c8fc3c945ca23",
     "grade": false,
     "grade_id": "cell-d1fa642d6113136c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values in each column:\\n\")\n",
    "X_tr[cat_cols].isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5lDSZvIWW2z4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83c0b8e39be5651b10a7b2f4b45840f4",
     "grade": false,
     "grade_id": "cell-221451761606d6cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Missing values\n",
    "\n",
    "<!-- [Missing values recap] - missing values may occir / appear in the data for various reasons. -->\n",
    "\n",
    "Most of the classical models are unable to handle missing values. Thus, we need to remove them from the data or fill with some value.\n",
    "\n",
    "There exist various strategies to deal with missing values. If some column or row lacks a significant part of the data, it may be reasonable to completely drop it. However, in most cases, this leads to significant information loss. Instead, missing values can be filled with some constant (such as 0 or an empty string), or a statistic of the corresponding feature (such as mean, median, or mode). When filling missing values, one may also create an additional feature that indicates whether the value was present in the original data. This can be informative because sometimes the absence of a feature value can be a feature in itself.\n",
    "\n",
    "\n",
    "<!-- Однако в большинстве случаев это приводит к значительной потере информации. Вместо этого, пропущенные значения могут быть заполнены некоторой константой, например нулем или пустой строкой, или статистикой по соответствующему признаку (например, средним, медианой или модой). При заполнении пропущенных значений, может быть также полезно создавать дополнительный признак-индикатор, указывающий, имелось ли это значение в изначальных данных. Так как иногда отсутствие значения само по себе может являться признаком. -->\n",
    "\n",
    "<!-- - (the most simple) just remove the feature that contains missing values and / or objects that have many empty fiels,\n",
    "- fill missing values with some constant value, for example, 0, -1 or an empty string\n",
    "- fill missing values with some statistic of the feature (mean, median, mode, etc)\n",
    "- ... -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KJ1A4hVqX9em",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4a39da99f90fb3612b23c08f295f11f",
     "grade": false,
     "grade_id": "cell-c31cb9e7c7658e67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.1** [1 pt]  <a class=\"anchor\" id=\"task1\"></a>\n",
    "\n",
    "Write a function that removes objects from the dataset, if they have more than **50%** of missing values.\n",
    "- Use pandas `isnull()` method to find missing (NaN) values. See example above.\n",
    "- Compute number of missing values in each row. Use argument `axis` to specify the dimention of summation. \n",
    "- From both the object matrix `X` and the target variable `y`, select a subset of rows with **at most 50%** of missing values.\n",
    "- See [this pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing) if you struggle to implement this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "FyaDreYvWVwk",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcd200f643492e0a164037f027aac9d0",
     "grade": false,
     "grade_id": "cell-6d8dfb21a0185f75",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_uninformative_objects(X, y):\n",
    "    # your code here\n",
    "    \n",
    "    return X_subset, y_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2w48l163XfKA",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "077e5151a2a0691285a36b507020c667",
     "grade": true,
     "grade_id": "cell-979228e097fcc15e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST drop_uninformative_objects function\n",
    "A = pd.DataFrame(np.array([\n",
    "    [0, 3, np.nan],\n",
    "    [4, np.nan, np.nan],\n",
    "    [np.nan, 6, 7],\n",
    "    [np.nan, np.nan, np.nan],\n",
    "    [5, 5, 5],\n",
    "    [np.nan, 8, np.nan],\n",
    "]))\n",
    "b = pd.Series(np.arange(6))\n",
    "A_subset, b_subset = drop_uninformative_objects(A, b)\n",
    "\n",
    "assert A_subset.shape == (3, 3)\n",
    "assert b_subset.shape == (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yf3BYsH11CkW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b06f207ac154ac7520bd99b86db8e2f",
     "grade": false,
     "grade_id": "cell-880acd9fa0c54f24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we can use this function to drop rows with too many missing values from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hlW1fThA1NJ8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7006dbc1be1c7aab8b925b1b6b63784",
     "grade": false,
     "grade_id": "cell-8fa260291b829ac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_tr, y_tr = drop_uninformative_objects(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MN4iLoAG1U0s",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d4f985742ac61db06866dc7df0c39c1",
     "grade": false,
     "grade_id": "cell-3014d9843873ca75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For categorical features, let's basically fill missing values with a constant value - an \"Unknown\" string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PRx5qUP51T24",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "922fc9f68c83aeb16919331c729ab905",
     "grade": false,
     "grade_id": "cell-836f59811bf6c15f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_tr[cat_cols] = X_tr[cat_cols].fillna(\"Unknown\")\n",
    "X_te[cat_cols] = X_te[cat_cols].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "brmXt9gnavYO",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf492ee39a91ecd9e97b0534d2952898",
     "grade": false,
     "grade_id": "cell-2d5298936e2bfc9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.2** [2 pts]  <a class=\"anchor\" id=\"task2\"></a>\n",
    "\n",
    "For numerical features, impelement a class that fills missing values in each feature with its mean value.\n",
    "\n",
    "- Implement a `fit` method that takes as input feature matrix `X` and computes and saves **mean** value of each feature (assuming that all the features are numerical). The feature matrix can be either `pd.DataFrame`, or `np.ndarray`.\n",
    "- Implement a `transform` method that takes as input feature matrix `X` and replaces NaN values in each feature with a corresponding mean value and returns a transformed feature matrix. \n",
    "\n",
    "<!-- Note, that `fit` method also takes target variable `y` as argument. It is needed for compatibility with other ML staff, however, in this class target variable will not be used, and you can just leave it equal to `None`. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "EAKkPXn7XMEO",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bae20da5eeb2357a387ec672d838ddb",
     "grade": false,
     "grade_id": "cell-197464a29b5d7c45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class MeanImputer(BaseEstimator, TransformerMixin): \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute and save mean value of each feature in the feature matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        y : pd.Series or numpy.ndarray of shape (n_samples,) (default : None)\n",
    "            Target values. Optional.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        self.column_mean = []\n",
    "        for i in range(X.shape[1]):\n",
    "            if type(X) is not np.ndarray:\n",
    "                X_i = X.values[:, i]\n",
    "            else:\n",
    "                X_i = X[:, i]\n",
    "                \n",
    "            # calculate mean for each column and add to the list `self.column_mean`\n",
    "            # your code here\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Fills missing values in each feature with a corresponding mean value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : array-like of shape (n_samples, n_features)\n",
    "            Transformed feature matrix. \n",
    "        \"\"\"\n",
    "\n",
    "        X_transformed = []\n",
    "        for i in range(len(self.column_mean)):\n",
    "            if type(X) is not np.ndarray:\n",
    "                X_i = X.values[:, i]\n",
    "            else:\n",
    "                X_i = X[:, i]\n",
    "                \n",
    "            # Fill missing values using mean values \n",
    "            # your code here\n",
    "            \n",
    "        \n",
    "        X_transformed = np.column_stack(X_transformed)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "w7kCQG5Jwf82",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93431062a16d1ba48ec132f8f52476ce",
     "grade": true,
     "grade_id": "cell-475729f5ca0dd035",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST MeanImputer class\n",
    "A = np.array([\n",
    "    [0, np.nan, 3],\n",
    "    [np.nan, 6, 7],\n",
    "    [np.nan, 8, np.nan],\n",
    "])\n",
    "mean_imp = MeanImputer()\n",
    "mean_imp.fit(A)\n",
    "print(mean_imp.column_mean)\n",
    "\n",
    "assert mean_imp.column_mean == [0.0, 7.0, 5.0], \"Computed mean values are incorrect.\"\n",
    "mean_imp.fit(pd.DataFrame(A))\n",
    "A_transformed = mean_imp.transform(pd.DataFrame(A))\n",
    "assert ~np.isnan(A_transformed).any(), \"Transformed feature matrix still contains NaNs.\"\n",
    "assert np.allclose(A_transformed, \n",
    "                   [[0., 7., 3.],\n",
    "                    [0., 6., 7.],\n",
    "                    [0., 8., 5.]]), \"Filled values are incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9DYQPUz8EG2t",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cf89bd227ed85037eab154de71d90c1",
     "grade": false,
     "grade_id": "cell-ca38bf936be8c65d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Numerical features\n",
    "\n",
    "From the histograms of numerical variables, you can also see that different features have different value ranges. However, many machine learning models, in particular linear models and kNN algorithm, are sensitive to feature scales. Therefore, it is a good practice to scale all the numeric variables to the same range before training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ghgohzq7XBLa",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9c6a77793d29e05765e9174911b0c55",
     "grade": false,
     "grade_id": "cell-9c03b67ed70742bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.3** [2 pts]  <a class=\"anchor\" id=\"task3\"></a>\n",
    "\n",
    "For numerical features, impelement a class that normalizes each feature values by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "- Implement a `fit` method that takes as input feature matrix `X` and computes and saves **mean** and **std** values of each feature (assuming that all the features are numerical). The feature matrix can be either `pd.DataFrame`, or `np.ndarray`.\n",
    "- Implement a `transform` method that takes as input feature matrix `X` and normalizes each column using precomputed mean and std values for this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "H0-vXGv6Vc0c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dff6a4accc5a2e552a3a0564125d983d",
     "grade": false,
     "grade_id": "cell-e06a079c48b613eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute and save mean and standard deviation values \n",
    "        of each feature in the feature matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        y : pd.Series or numpy.ndarray of shape (n_samples,) (default : None)\n",
    "            Target values. Optional.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        self.column_mean = []\n",
    "        self.column_std = []\n",
    "        for i in range(X.shape[1]):\n",
    "            # extract column values \n",
    "            if type(X) is not np.ndarray:\n",
    "                X_i = X.values[:, i]\n",
    "            else:\n",
    "                X_i = X[:, i]\n",
    "                \n",
    "            # compute mean and std\n",
    "            # your code here\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Normalizes values of each feature \n",
    "        by subtructing the corresponding precomputed feature mean \n",
    "        and dividing by the corresponding standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : array-like of shape (n_samples, n_features)\n",
    "            Transformed feature matrix. \n",
    "        \"\"\"   \n",
    "        X_transformed = []\n",
    "        \n",
    "        for i in range(len(self.column_mean)):\n",
    "            # extract column values \n",
    "            if type(X) is not np.ndarray:\n",
    "                X_i = X.values[:, i]\n",
    "            else:\n",
    "                X_i = X[:, i]\n",
    "                \n",
    "            # your code here\n",
    "            \n",
    "            \n",
    "        X_transformed = np.column_stack(X_transformed)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Rt1uBpln7bhq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed5d38739ad6a26ae94defd66ad74f9d",
     "grade": true,
     "grade_id": "cell-34a318dd0934c18e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST Scaler class\n",
    "A = np.arange(9).reshape(3, 3)\n",
    "scaler = Scaler()\n",
    "scaler.fit(A)\n",
    "\n",
    "assert np.allclose(scaler.column_mean, [3.0, 4.0, 5.0]), \"Computed mean values are incorrect.\"\n",
    "assert np.allclose(scaler.column_std, [2.44949, 2.44949, 2.44949]), \"Computed std values are incorrect.\"\n",
    "scaler.fit(pd.DataFrame(A))\n",
    "A_transformed = scaler.transform(pd.DataFrame(A))\n",
    "assert np.allclose(A_transformed, \n",
    "                   [[-1.22474, -1.22474, -1.22474],\n",
    "                    [ 0.     ,  0.     ,  0.     ],\n",
    "                    [ 1.22474,  1.22474,  1.22474]]), \"Scaled values are incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UqCmMJmAFL5J",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d730bf5bebe05dfc0cd4e31419fb356",
     "grade": false,
     "grade_id": "cell-fc3e9de0b5f5f60f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " ### Model training and validation\n",
    "\n",
    "Now, we will train and evaluate the first regression model for diamond price prediction using only the numerical features. We will use **k-nearest neighbors** algorithm to build the model, and mean absolute error metric to evaluate its predictive performance. \n",
    "\n",
    "It is important that for correct and fair training, the test set should not be used at any stage of model building. Therefore, statistics for feature preprocessing have to be estimated only on the training sample. For example, the mean and standard deviation of each column should be computed on the training set and then used to normalize the features in both the training and test sets.\n",
    "\n",
    "`Sklearn` contains ready-to-use feature transformer classes, with the `fit` method to estimate parameters on the training set and the `transform` method to actually apply the transformation to training and test sets. `MeanImputer` and `Scaler` classes, that you have implemented previously, are also compatible with other sklearn transformers.\n",
    "\n",
    "There also exist several tools to combine transformers and estimators into a single pipeline:\n",
    "- `ColumnTransformer`\n",
    "- `Pipeline`\n",
    "- `make_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VTlqz02ohdH6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe3f39e578663e2f61def81b20fb80ae",
     "grade": false,
     "grade_id": "cell-2d220bdb23396161",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define transformations for numerical columns using MeanImputer and Scaler\n",
    "num_transformers_pipe = make_pipeline(\n",
    "    MeanImputer(),\n",
    "    Scaler(),\n",
    ")\n",
    "\n",
    "# defien column transformer (we drop non-numerical features)\n",
    "col_transformer = ColumnTransformer([\n",
    "    (\"num_col_transformer\", num_transformers_pipe, num_cols),\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "# define a Pipeline with 2 steps: 1. transform columns; 2. Apply the model\n",
    "model = Pipeline([\n",
    "    (\"col_transformer\", col_transformer),\n",
    "    (\"estimator\", KNeighborsRegressor())\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlVc8ZgdxxC2"
   },
   "source": [
    "\n",
    "---\n",
    "**Task 1.4** [1 pt]  <a class=\"anchor\" id=\"task4\"></a>\n",
    "\n",
    "Implement a function to evaluate performance of a given model both on the train and test sets. The function should\n",
    "- take as input a model (possibly a pipeline), as well as feature matrix and target variable for both train and test sets,\n",
    "- fit the model on the train set,\n",
    "- compute model predictions for train and test sets,\n",
    "- assess the quality of both predictions with mean absolute error (MAE) metric,\n",
    "- return train and test MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Vj6GGiGlZmyE",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fd47f2e6cb915394e289233b9d373b5",
     "grade": false,
     "grade_id": "cell-3b2863b8ca643ff7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te):\n",
    "    # your code here\n",
    "    \n",
    "    return mae_tr, mae_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6bd4b78dcc725f4300ca67f8bf08df4",
     "grade": false,
     "grade_id": "cell-6e6bda66a036866a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluate performance of the model based on numerical features only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mOGOgRqMzxfq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4314d25a9d82ef000721842616cca4ef",
     "grade": true,
     "grade_id": "cell-681232043cba68a4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST evaluate_model function\n",
    "mae_tr, mae_te = evaluate_model(model, X_tr, y_tr, X_te, y_te)\n",
    "\n",
    "assert np.allclose(mae_tr, 684.50, rtol=1e-1)\n",
    "assert np.allclose(mae_te, 833.22, rtol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GytsZtgCz3nc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a8ef7952185183b555c6a4f911413ee",
     "grade": false,
     "grade_id": "cell-1aa7dd77ca19a40c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "models_scores = {}\n",
    "models_scores[\"num only\"] = {\"train\" : mae_tr, \"test\" : mae_te}\n",
    "print(\"Train MAE = %.2f\" % mae_tr)\n",
    "print(\"Test MAE = %.2f\" % mae_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNxGoLt_9ufB"
   },
   "source": [
    "### Categorical features\n",
    "\n",
    "<!-- [categorical features recap?] -->\n",
    "\n",
    "Most of the machine learning models cannot work with categorical features directly in their original form. However, there are several ways to represent them in an alternative form (encode). We will consider and compare two options for encoding categorical features - **one-hot encoding** and **mean target encoding**.\n",
    "\n",
    "Ohe-hot-encoding is one of the most popular and straight-forward ways. In this case each categorical feature is replaced by a set of binary features for each category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsB7Qg9xDnZ8"
   },
   "source": [
    "`Sklearn` has ready-to-use implementation of OneHotEncoder transformer (as well as many other transformers). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1607627239273,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "wa_lzhVZ19a8",
    "outputId": "911c3e26-a674-42e3-fdc3-67bece89697b"
   },
   "outputs": [],
   "source": [
    "print(\"Categorical variable before one-hot encoding:\")\n",
    "display(X_tr[[\"cut\"]].head())\n",
    "\n",
    "print(\"Categorical variable after one-hot encoding:\")\n",
    "ohenc = OneHotEncoder(sparse=False)\n",
    "ohenc.fit_transform(X_tr[[\"cut\"]])\n",
    "display(pd.DataFrame(ohenc.fit_transform(X_tr[[\"cut\"]]), \n",
    "                     index=X_tr.index, columns=ohenc.categories_).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f02ca58e3db8f6da1476fa38d717793",
     "grade": false,
     "grade_id": "cell-56fa2e7770c46be2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 1.5** [1 pts]  <a class=\"anchor\" id=\"task5\"></a>\n",
    "\n",
    "Let us create a 2-step pipeline:\n",
    "\n",
    "1. Column Transformer, which\n",
    "    - Apply one-hot encoding to categorical feature\n",
    "    - Apply `num_transformers_pipe` (defined above) to numerical feature\n",
    "2. A regression model\n",
    "\n",
    "Please, finish the definition of the `col_transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "MDnEd4SOh85b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d21ea222367a99d6ee6501fbdbc5d0f",
     "grade": false,
     "grade_id": "cell-e0f59c3b9570aa2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "col_transformer = ColumnTransformer([\n",
    "    # your code here\n",
    "    \n",
    "], remainder='drop')\n",
    "\n",
    " \n",
    "\n",
    "model = Pipeline(steps = [\n",
    "    (\"col_transformer\", col_transformer),\n",
    "    (\"estimator\", KNeighborsRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 15584,
     "status": "ok",
     "timestamp": 1607627263188,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "rqI9PU_Qh85b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b9884a731f581fa324d4af3d6f7448e",
     "grade": true,
     "grade_id": "cell-936346eac885f257",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f5333293-db33-4e27-8eed-dd996982ae93"
   },
   "outputs": [],
   "source": [
    "assert len(col_transformer.transformers) == 2, '`col_transformer` should have 2 groups - for numerical nad categoical features'\n",
    "num_pipe1 = isinstance(col_transformer.transformers[0][1], Pipeline)\n",
    "num_pipe2 = isinstance(col_transformer.transformers[1][1], Pipeline)\n",
    "assert  num_pipe1 | num_pipe2, 'Numerical features should be transformed via  `num_transformers_pipe`'\n",
    "\n",
    "cat_pipe1 = isinstance(col_transformer.transformers[0][1], OneHotEncoder)\n",
    "cat_pipe2 = isinstance(col_transformer.transformers[1][1], OneHotEncoder)\n",
    "assert  cat_pipe1 | cat_pipe2, 'Categorical features should be transformed via  `OneHotEncoder`'\n",
    "\n",
    "mae_tr, mae_te = evaluate_model(model, X_tr, y_tr, X_te, y_te)\n",
    "models_scores[\"num & cat (one-hot)\"] = {\"train\" : mae_tr, \"test\" : mae_te}\n",
    "print(\"Train MAE = %.2f\" % mae_tr)\n",
    "print(\"Test MAE = %.2f\" % mae_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TZt6bhT-X9en",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "344718b63194e077f3455e10229ed0cc",
     "grade": false,
     "grade_id": "cell-105edfbad47a5ceb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Mean target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTWuCDxtdVH1"
   },
   "source": [
    "Another way to encode categorical variables in **mean target encoding**. The idea behind the approach is that each of the categories is replaced by the average value of the target variable for objects from that category. Thus, the categorical feature can be encoded with a single numerical vector.\n",
    "\n",
    "<!-- [overfirtting and adding noise to the target variable] -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmfJGLDsLzny"
   },
   "source": [
    "---\n",
    "**Task 1.6** [2 pts]  <a class=\"anchor\" id=\"task6\"></a>\n",
    "\n",
    "Implement mean target encoding transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "gW1XwoL3O8zu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed906ce2bf5d0a768875d29b5088729d",
     "grade": false,
     "grade_id": "cell-9ca2a5792628cf4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class MeanTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encode categorical features as a numerical vector \n",
    "    of mean target values for each category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    noise_level : float, default=0.01\n",
    "        The variance of Gaussian noise to be added to the target variable \n",
    "        before computing the mean. Larger values lead to less accurate \n",
    "        mean values for categories but prevent overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 noise_level=0.01):\n",
    "        self.noise_level = noise_level\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\"\n",
    "        For each feature in the feature matrix, \n",
    "        compute and save mean target value for each category. \n",
    "        \n",
    "        All the values are stored in the dictionary `self.column_category_mean`\n",
    "        Keys - column names. Values - dictionary with unique categories and corresponding means\n",
    "        \n",
    "        Example for 2 categorical features: `district`, `city_type`  \n",
    "        self.column_category_mean = {\n",
    "                                    'distric': {'central': 2000, \n",
    "                                                 'south': 1000, \n",
    "                                                 'north': 1500},\n",
    "                                    'city_type': {'capital': 2100,\n",
    "                                                  'small city': 1700,\n",
    "                                                  'village': 1200}\n",
    "                                      }\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        y : pd.Series or numpy.ndarray of shape (n_samples,) (default : None)\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get target and add noise to it\n",
    "        if type(y) is not np.ndarray:\n",
    "            target = y.values.copy()\n",
    "        else:\n",
    "            target = y.copy()\n",
    "        if self.noise_level > 0:\n",
    "            noise = np.random.randn(target.size) * (self.noise_level * target.std())\n",
    "            target = target + noise\n",
    "\n",
    "\n",
    "        self.column_category_mean = {}\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            # get column name (`col`) and the values (`X_i`)\n",
    "            if type(X) is not np.ndarray:\n",
    "                col = X.columns[i]\n",
    "                X_i = X[col].values\n",
    "            else:\n",
    "                col = i\n",
    "                X_i = X[:, col]\n",
    "                \n",
    "            # your code here\n",
    "            \n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        For each feature in the feature matrix,\n",
    "        replaces each category with a corresponding precomuted \n",
    "        mean target value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame or numpy.ndarray of shape (n_samples, n_features)\n",
    "            Feature matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : array-like of shape (n_samples, n_features)\n",
    "            Transformed feature matrix. \n",
    "        \"\"\"\n",
    "\n",
    "        X_transformed = []\n",
    "        for col in self.column_category_mean:\n",
    "            # extract column values \n",
    "            if type(X) is not np.ndarray:\n",
    "                X_i = X[col].values\n",
    "            else:\n",
    "                X_i = X[:, col]\n",
    "                \n",
    "            # encode categories of X_i using `self.column_category_mean`. And append the result to `X_transformed`\n",
    "            # your code here\n",
    "            \n",
    "                \n",
    "        X_transformed = np.column_stack(X_transformed)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fOrYsWbrnevP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a98007201d60884313451f3e03403f1",
     "grade": true,
     "grade_id": "cell-5b7539f99c11a112",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST MeanTargetEncoder\n",
    "\n",
    "# toy dataset\n",
    "dist = ['south', 'central', 'south','central','north','north','central']\n",
    "city = ['small city', 'small city', 'capital', 'village', 'capital', 'capital', 'village']\n",
    "price = np.array([1000, 2000, 3000, 2800, 1200, 1700, 2100])\n",
    "X = pd.DataFrame({'district':dist, 'city_type': city})\n",
    "\n",
    "# mean target encoder\n",
    "enc = MeanTargetEncoder(noise_level=0.)\n",
    "enc.fit(X, price)\n",
    "X_encoded = pd.DataFrame(enc.transform(X), columns = X.columns)\n",
    "display(X_encoded)\n",
    "\n",
    "assert np.all(X_encoded.columns == X.columns)\n",
    "assert np.allclose(np.sort(X_encoded.district.unique()), np.array([1450., 2000., 2300.])), 'Column `distric` from the tay dataser was not encoded correctly'\n",
    "assert np.allclose(np.sort(X_encoded.city_type.unique()), np.array([1500., 1966.66666667, 2450.])), 'Column `city_type` from the tay dataser was not encoded correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHJ_QB9MOLSJ"
   },
   "outputs": [],
   "source": [
    "# use Mean Target encoder instead of OHE\n",
    "cat_mean_target_enc = make_pipeline(\n",
    "    MeanTargetEncoder(noise_level=0.01),\n",
    "    StandardScaler(),\n",
    ")\n",
    "\n",
    "col_transformer = ColumnTransformer([\n",
    "    (\"num_col_transformer\", num_transformers_pipe, num_cols),\n",
    "    (\"cat_mean_target\", cat_mean_target_enc, cat_cols),\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"col_transformer\", col_transformer),\n",
    "    (\"estimator\", KNeighborsRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3694,
     "status": "ok",
     "timestamp": 1607627346071,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "XY944ZsuOLUr",
    "outputId": "b3e38b9c-44fb-4d19-9161-26935507cd20"
   },
   "outputs": [],
   "source": [
    "mae_tr, mae_te = evaluate_model(model, X_tr, y_tr, X_te, y_te)\n",
    "models_scores[\"num & cat (mean target)\"] = {\"train\" : mae_tr, \"test\" : mae_te}\n",
    "print(\"Train MAE = %.2f\" % mae_tr)\n",
    "print(\"Test MAE = %.2f\" % mae_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51bv9frfj8r7"
   },
   "source": [
    "\n",
    "### Ordinal categorical variables\n",
    "\n",
    "In some cases, categorical variables may have ordinal values. For example, in our data, the attribute \"cut\" has values Fair, Good, Very Good, Premium, and Ideal. We can assume that these values are \"ordered\" and encode them with the numbers 1, 2, 3, 4 and 5 respectively. Thus, 1 means \"worst\" value (Fair) and 5 means \"best\" (Ideal) and 0 can be used to encode \"Unknown\" (missing) values. \n",
    "This can provide additional information to the model and improve quality, especially for linear models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1607628029759,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "oga5ppKgUXbN",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e327db3e284a6930209a8a2ea2bea3c",
     "grade": false,
     "grade_id": "cell-64315b20a5d33b2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1952b71d-dec7-4760-9279-c2d137e2d2e8"
   },
   "outputs": [],
   "source": [
    "# separate columns with ordinal features from other categorical features\n",
    "ord_cols = [\"cut\"]\n",
    "cat_cols = list(set(cat_cols) - set(ord_cols))\n",
    "print(cat_cols, ord_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "N6CHoFTwTw5Y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cee4612c68dc2b8790e6715fcbb2d9ee",
     "grade": false,
     "grade_id": "cell-cc95e138609d2b62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create lists of categories for each ordinal feature\n",
    "# (categories should be ordered from minimal to maximal)\n",
    "ord_col_categories = [[\n",
    "    \"Unknown\",\n",
    "    \"Fair\",\n",
    "    \"Good\",\n",
    "    \"Very Good\",\n",
    "    \"Premium\",\n",
    "    \"Ideal\",          \n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D91_HQC-VVHv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "caf7e2dd160b65b3970f58f95ebfde5e",
     "grade": false,
     "grade_id": "cell-2e276550a6ac19e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# defien ordinal encoder\n",
    "ord_enc = make_pipeline(\n",
    "    OrdinalEncoder(categories=ord_col_categories),\n",
    "    StandardScaler(),\n",
    ")\n",
    "\n",
    "# combine all column transforms\n",
    "col_transformer = ColumnTransformer(transformers = [\n",
    "    (\"num_col_transformer\", num_transformers_pipe, num_cols),\n",
    "    (\"cat_one_hot\",  OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"ord_enc\", ord_enc, ord_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "model = Pipeline(steps = [\n",
    "    (\"col_transformer\", col_transformer),\n",
    "    (\"estimator\", KNeighborsRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 12131,
     "status": "ok",
     "timestamp": 1607628367275,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "ONau_qTiV03g",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5d19b26cb3f97c70a824ef8a9f85dd5",
     "grade": false,
     "grade_id": "cell-6f46b5edf13966c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e304db04-6a23-416f-d244-43a95683ef0a"
   },
   "outputs": [],
   "source": [
    "# evaluate model performance\n",
    "mae_tr, mae_te = evaluate_model(model, X_tr, y_tr, X_te, y_te)\n",
    "models_scores[\"num, cat & ord\"] = {\"train\" : mae_tr, \"test\" : mae_te}\n",
    "print(\"Train MAE = %.2f\" % mae_tr)\n",
    "print(\"Test MAE = %.2f\" % mae_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfKK-MVBmVcw"
   },
   "source": [
    "\n",
    "### Feature engineering\n",
    "\n",
    "We can also use the original features to derive new, more complex or informative features on their basis. \n",
    "\n",
    "For example, in our data we have three features \"x\", \"y\" and \"z\" and can assume that they correspond to measurements of the diamond size along three axes. In this case, product of these three features, that is, the approximate volume, can also be a useful feature for predicting the value of a diamond.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1607628375659,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "Rf469JT_WkBT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09ab8377a5080512c8166b45c49eea79",
     "grade": false,
     "grade_id": "cell-f832e432b56fb3b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "15659c24-9a68-4f85-f251-9874b35c9d44"
   },
   "outputs": [],
   "source": [
    "X_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a7f6caa8f8bb7b4f035e1300e669318",
     "grade": false,
     "grade_id": "cell-db8f89c8a7b29fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Task 1.7** [1 pts]  <a class=\"anchor\" id=\"task7\"></a>\n",
    "\n",
    "Add `volume` feature as a product of `x`, `y`, `z` features (to both train and test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1607629310821,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "BV4W-CC_Wp5g",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ebcac47e7dca340744fcbc810082ddc",
     "grade": false,
     "grade_id": "cell-fbd57bcfb8f51740",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "8f2a7387-ab3e-4a22-fd08-6e1986378988"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4313acbcefdb8839769de3ef36163ada",
     "grade": true,
     "grade_id": "cell-1982ab69808024bc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Distribution of created features:\")\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "X_tr[[\"volume\"]].hist(bins=20, grid=True, ax=ax[0])\n",
    "ax[0].set_title('Volume on train')\n",
    "X_te[[\"volume\"]].hist(bins=20, grid=True, ax=ax[1]);\n",
    "ax[0].set_title('Volume on test')\n",
    "\n",
    "assert np.allclose(X_tr[[\"volume\"]].min(), 0)\n",
    "assert np.allclose(X_tr[[\"volume\"]].max(), 838.5024)\n",
    "assert np.allclose(X_tr[[\"volume\"]].mean(), 129.82611)\n",
    "\n",
    "assert np.allclose(X_te[[\"volume\"]].min(), 0)\n",
    "assert np.allclose(X_te[[\"volume\"]].max(), 698.455296)\n",
    "assert np.allclose(X_te[[\"volume\"]].mean(), 129.874711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1607629551794,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "OMjiPVlqYBgm",
    "outputId": "ce00c2e9-577a-4167-bb2c-247ad63271f5"
   },
   "outputs": [],
   "source": [
    "# update list of numerical features\n",
    "num_cols = X_tr.columns[np.logical_or(X_tr.dtypes == \"int64\", X_tr.dtypes == \"float64\")].tolist() #\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHoZFnQTYb4L"
   },
   "outputs": [],
   "source": [
    "# define columns transforms (nothing cahnges for ordinal and categorical features, but the list of numerical features was updated)\n",
    "col_transformer = ColumnTransformer(transformers = [\n",
    "    (\"num_col_transformer\", num_transformers_pipe, num_cols),\n",
    "    (\"cat_one_hot\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"ord_enc\", ord_enc, ord_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "model = Pipeline(steps = [\n",
    "    (\"col_transformer\", col_transformer),\n",
    "    (\"estimator\", KNeighborsRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12592,
     "status": "ok",
     "timestamp": 1607629564589,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "Rm6N9GCgYnmt",
    "outputId": "9326b371-1d8b-4a7b-f9c8-2c2f1e0073c9"
   },
   "outputs": [],
   "source": [
    "# evaluate model performance\n",
    "mae_tr, mae_te = evaluate_model(model, X_tr, y_tr, X_te, y_te)\n",
    "models_scores[\"num, cat & ord + engineering\"] = {\"train\" : mae_tr, \"test\" : mae_te}\n",
    "print(\"Train MAE = %.2f\" % mae_tr)\n",
    "print(\"Test MAE = %.2f\" % mae_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch8dhnW4VCsb"
   },
   "source": [
    "Finally, we may look how model performance on train and test sets have changed depending on the used features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 811,
     "status": "ok",
     "timestamp": 1607631874106,
     "user": {
      "displayName": "Марина Поминова",
      "photoUrl": "",
      "userId": "04196186528550871600"
     },
     "user_tz": -180
    },
    "id": "7-0Y18TrOpDt",
    "outputId": "704067c0-10ad-4874-f787-881d1a1801a3"
   },
   "outputs": [],
   "source": [
    "models = list(models_scores.keys())\n",
    "train_scores = [models_scores[model][\"train\"] for model in models]\n",
    "test_scores = [models_scores[model][\"test\"] for model in models]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_scores)\n",
    "plt.plot(test_scores)\n",
    "plt.scatter(range(len(models)), train_scores, label=\"train\")\n",
    "plt.scatter(range(len(models)), test_scores, label=\"test\")\n",
    "plt.xticks(range(len(models)), models, rotation=30)\n",
    "plt.xlabel(\"Features used\", fontdict={\"size\" : 12})\n",
    "plt.ylabel(\"Model's MAE score\", fontdict={\"size\" : 12})\n",
    "plt.title(\"Dependence of model's score on the features used\", fontdict={\"size\" : 14})\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "w9-PEr8QWkyC",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36bec9522a440ff54b8d87534ed41320",
     "grade": false,
     "grade_id": "cell-a5cb5f37ad4de405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Problem 2. Text data analysis <a class=\"anchor\" id=\"problem2\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqUtBM9TWkyC"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmRn3nSmWkyC"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMe0cbA7WkyC"
   },
   "source": [
    "In this part, we will solve the problem of prediction price of the wine based on text description of its characteristics. When working with text features, you have to first transform them into a vector form (vectorize) so that the model can process them. We wil consider two vectorization methods, \n",
    "* **Bag-of-Words** \n",
    "* **Tf-Idf**.\n",
    "\n",
    "Let us read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b11-5tr8WkyD"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"winemag-data_first50k.csv\", index_col=0)\n",
    "# drop rows with unknown price\n",
    "df = df.loc[df[\"price\"].notnull()]\n",
    "\n",
    "y = df[\"price\"]\n",
    "X = df.drop([\"price\"], axis=1)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKyKy2uYWkyD"
   },
   "source": [
    "### Bag-of-Words vectorization\n",
    "\n",
    "Bag-of-Words is one of the most obvious and straight-forward for the text descriptions vectorization.\n",
    "In this case, we just create a feature column for each word appearing in the texts of the dataset. If the particular word is presented in the text description of a particular object, the value of the corresponding feature will be equal to 1, else - equal to 0. It is also possible to set the value of the feature to the number of times that the corresponding word has appeared in the object description.\n",
    "\n",
    "In `sklearn`, you can obtain bag-of-words vector representation of the text using the `CountVectorizer` transformer. Note, that in this case value of the resulting features will be equal to the number of times the corresponding word appeared in the object description text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy5tOJmIWkyD"
   },
   "source": [
    "---\n",
    "**Task 2.1** [2 pts]  <a class=\"anchor\" id=\"task2_1\"></a>\n",
    "\n",
    "Before vectorization, we need to prepare text descriptions. Write a function to clear the text from all non-alphabetic characters and convert it to lowercase. \n",
    "\n",
    "**Hint**\n",
    "- `.lower()` method is used to conver any string to the lower case. E.g `'Hello'.lower()` retuns `'hello'`\n",
    "- Use `re.sub()` to replace all non-alphabetic characteer with empty string. You cen read more about it in the [documentation](https://docs.python.org/3/library/re.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "a6B52gVAWkyD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e42b78d3bf400ce404c166bb937a938e",
     "grade": false,
     "grade_id": "cell-25e2882c43cf7177",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def prepare_text(text):\n",
    "    \"\"\"\n",
    "    Converts the text to lowercase and removes all the non-alphabetic characters\n",
    "    (the cleared text should contain only letters from a to z and space symbols).\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "W6Bl5EqtWkyD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f59319ff509c741a26c7b8cf3fd79cde",
     "grade": true,
     "grade_id": "cell-491389d3bcfc9e80",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST prepare_text\n",
    "processed = prepare_text('CLEAR it!!!')\n",
    "assert 'clear it' == processed\n",
    "\n",
    "assert prepare_text('AAaa..a a') == 'aaaaa a'\n",
    "assert prepare_text('!How are you?') == 'how are you'\n",
    "assert prepare_text('m.y. w.o.r.d') == 'my word'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e74791f161519220b7b61dc0123d08e4",
     "grade": false,
     "grade_id": "cell-5ae370e13c04fedc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Task 2.2** [1 pt]  <a class=\"anchor\" id=\"task2_2\"></a>\n",
    "\n",
    "Apply `prepare_text` function to the `description` feature column in the datasets `X_tr` and `X_te`. \n",
    "\n",
    "**Hint** Use the method `apply`from [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "AD0_XhF9WkyD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970aa5dffca029c5f592e6837f79",
     "grade": false,
     "grade_id": "cell-2123ec0210d09ca6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "024cb3a584ad069bd8e1559c0c87db26",
     "grade": true,
     "grade_id": "cell-b100f9b1b744ac8f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure that this string does not contain upper case letter and non-alphabetical characters\n",
    "print(X_tr[\"description\"][0])\n",
    "\n",
    "assert X_tr[\"description\"][100].lower() == X_tr[\"description\"][100]\n",
    "assert X_tr[\"description\"][101].lower() == X_tr[\"description\"][101]\n",
    "\n",
    "assert X_te[\"description\"][10].lower() == X_te[\"description\"][10]\n",
    "assert X_te[\"description\"][11].lower() == X_te[\"description\"][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task 2.3** [1 pt]  <a class=\"anchor\" id=\"task2_3\"></a>\n",
    "\n",
    "Perform the **Bag-of-Words vectorization** of the texts in `description` column, train the linear regression model on the obtained numerical features and evaluate its mean absolute error on the test set.\n",
    "\n",
    "- Use `CountVectorizer` from sklearn to perform Bag-of-Words vectorization. \n",
    "    - Use the argument `min_df=0.001` to remove the words which appear in less then 0.1% of the documents\n",
    "    - Read more about it the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- Fit the vectorizer using descriptions from the train dataset\n",
    "- Create `textfeats_tr` which contains transformed descriptions from the train dataset\n",
    "- Create `textfeats_te` which contains transformed descriptions from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "ynyw-HBeWkyD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ea64cff7380642e7626de42dfa994dd",
     "grade": false,
     "grade_id": "cell-6261a8396767b246",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(min_df=0.001)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xvAb-SeTWkyD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e8745e52f542093292e9c1d3019f185",
     "grade": true,
     "grade_id": "cell-4ae5c84cf3158079",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST bag-of-words vectorization\n",
    "print(textfeats_tr.shape)\n",
    "print(textfeats_te.shape)\n",
    "\n",
    "assert textfeats_tr.shape[1] == textfeats_te.shape[1]\n",
    "assert textfeats_tr.shape[1] == 2468\n",
    "assert len(bow_vectorizer.vocabulary_) == textfeats_tr.shape[1]\n",
    "assert len(bow_vectorizer.stop_words_) > 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate a `LinearRegression` model on the obtained features. We will use function `evaluate_model`, that you've created in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hD1810BBWkyD",
    "outputId": "f89fc250-d343-4d71-ad42-bc57fdde35ad"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "mae_tr, mae_te = evaluate_model(model, textfeats_tr, y_tr, \n",
    "                                textfeats_te, y_te)\n",
    "print(\"Train MAE = %.3f\" % mae_tr)\n",
    "print(\"Test MAE = %.3f\" % mae_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vppR-f07WkyD"
   },
   "source": [
    "### Tf-Idf vectorization\n",
    "\n",
    "Another way to deal with text data is TF-IDF - Term Frequency - Inverse Document Frequency vectorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLMvt3B2WkyD"
   },
   "source": [
    "---\n",
    "**Task 2.4** [1 pts]  <a class=\"anchor\" id=\"task2_4\"></a>\n",
    "\n",
    "Perform **Tf-Idf vectorization** of the cleaned description obtained in the previous step. \n",
    "\n",
    "\n",
    "- Use `TfidfVectorizer` from sklearn to perform Bag-of-Words vectorization. \n",
    "    - Use the argument `min_df=0.001` to remove the words which appear in less then 0.1% of the documents\n",
    "    - Read more about it the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- Fit the vectorizer using descriptions from the train dataset\n",
    "- Create `textfeats_tr` which contains transformed descriptions from the train dataset\n",
    "- Create `textfeats_te` which contains transformed descriptions from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "h8VQRr9mWkyD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc84671aed08f5d909931e4cfa4d7276",
     "grade": false,
     "grade_id": "cell-1253b9f2b4143df7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.001)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7ffdb26cea49488d40a23b6fbe822b9",
     "grade": true,
     "grade_id": "cell-5ce6a8403c6f1cb0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST bag-of-words vectorization\n",
    "print(textfeats_tr.shape)\n",
    "print(textfeats_te.shape)\n",
    "\n",
    "assert textfeats_tr.shape[1] == textfeats_te.shape[1]\n",
    "assert textfeats_tr.shape[1] == 2468\n",
    "assert len(tfidf_vectorizer.vocabulary_) == textfeats_tr.shape[1]\n",
    "assert len(tfidf_vectorizer.stop_words_) > 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to train and evaluate the linear regression model on the vectorized text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKwaDhPoWkyD",
    "outputId": "1e6b5f29-7cb4-4fd6-836c-513c9d307802"
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "mae_tr, mae_te = evaluate_model(model, textfeats_tr, y_tr, \n",
    "                                textfeats_te, y_te)\n",
    "print(\"Train MAE = %.3f\" % mae_tr)\n",
    "print(\"Test MAE = %.3f\" % mae_te)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQRY7nzsLHW7b9DxofAant",
   "collapsed_sections": [
    "xKyKy2uYWkyD",
    "zy5tOJmIWkyD",
    "vppR-f07WkyD"
   ],
   "name": "HW 1",
   "provenance": [
    {
     "file_id": "1wzx_Qny7A-fXUqfiWgn6q5qZJ3zAueKM",
     "timestamp": 1607432099194
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
