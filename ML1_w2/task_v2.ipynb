{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d25be3d3f3ca89cf476013f7c6de17c2",
     "grade": false,
     "grade_id": "cell-14d5b707a6e4bcc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7350b671f695e1d5131358c245a1ea8",
     "grade": false,
     "grade_id": "cell-93f4f1852b43f349",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "This is programming assignment for week 2. In this assignment you will be implementing gradient descent and weighted linear regression from scratch. \n",
    "\n",
    "Please, read all the notebook carefully and make sure that you understand not only the task, but the whole pipeline.\n",
    "\n",
    "### Grading\n",
    "The assignment is automatically graded. \n",
    "\n",
    "**Automatic grading**\n",
    "After you finish solving all the tasks restart the kernel (`kernel -> restart`) and click the button `Validate` to check that everything works as expected. Afterwards, you can submit your work.\n",
    "\n",
    "\n",
    "# Table of Contents:\n",
    "* [Problem 1.](#part1)  Gradient Descent\n",
    "     - [Task 1](#task1) [2 pts]\n",
    "     - [Task 2](#task2) [3 pts]\n",
    "     - [Task 3](#task3) [1 pts]\n",
    "     - [Task 4](#task4) [1 pt]\n",
    "\n",
    "   \n",
    "* [Problem 2](#part2). Weighted Linear Regression\n",
    "    - [Task 1](#task2_1) [1 pts]\n",
    "    - [Task 2](#task2_2) [1 pts]\n",
    "    - [Task 3](#task2_3) [1 pts]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Problem 1. Gradient Descent  <a class=\"anchor\" id=\"part1\"></a>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb11e98c45917c2d8b9aa17fa66501ca",
     "grade": false,
     "grade_id": "cell-0df849c71a227b9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_function(f, x_min=-10, x_max=10):\n",
    "    \"\"\"\n",
    "    Plot 2d function within specified range\n",
    "    \"\"\"\n",
    "    x_grid = np.linspace(x_min, x_max,  500)\n",
    "    plt.plot(x_grid, f(x_grid))\n",
    "    plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4d94885e940004b5f623a7c4fbf549c",
     "grade": false,
     "grade_id": "cell-105edfbad47a5ceb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Task 1.1** [2 pts] <a class=\"anchor\" id=\"task1\"></a>\n",
    "\n",
    "Consider a function $$f(x) = x^2 - 15\\sin \\left(\\tfrac{\\pi}{3}x\\right).$$ \n",
    "\n",
    "Implement funtions `f(x)` and `grad_f(x)`, which evaluate function and its gradient in any given point `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9b18c0bbbc80d2365a83060ba1f310a",
     "grade": false,
     "grade_id": "cell-3e9544fb90ef4e42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    Evaluates function `f(x) = x^2 - 15 \\sin(x * \\pi/3)`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input(s) to the function\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        Function `f`, evaluated at point(s) x\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "\n",
    "    \n",
    "def grad_f(x):\n",
    "    \"\"\"\n",
    "    Evaluates gradient of a function `f(x) = x^2 - 15 \\sin(x * \\pi/3)`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Point(s) at which gradient should be avalueated\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        Gradient of the function `f` evaluaed at point(s) x\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79ddaf02fcb387593a6972bc7e216ee3",
     "grade": true,
     "grade_id": "cell-15b86cfb16d50564",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST f\n",
    "assert f(0.) == 0.\n",
    "assert np.allclose(f(np.array([2.5, 7.5])), np.array([-1.25, 41.25]))\n",
    "\n",
    "assert np.allclose(f(np.arange(-10, 10, 1)), np.arange(-10, 10, 1)**2 - 15*np.sin(np.pi/3*np.arange(-10, 10, 1)))\n",
    "x_min = 1.33668375\n",
    "assert np.allclose(f(x_min), -12.9944407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9eed4f9940ce63ab67f73565b8d19cf8",
     "grade": true,
     "grade_id": "cell-84e9e0e99e209baf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST grad_f\n",
    "assert np.allclose(grad_f(0.), -15.7079)\n",
    "assert isinstance(grad_f(0.), float)\n",
    "\n",
    "assert np.allclose(grad_f(0), - 5*np.pi*np.cos(0)), 'Gradient at point 0 is wrong'\n",
    "assert np.allclose(grad_f(np.arange(-10, 10, 1)), 2*np.arange(-10, 10, 1) - 5*np.pi*np.cos(np.pi/3*np.arange(-10, 10, 1)))\n",
    "x_min = 1.33668375\n",
    "assert np.allclose(grad_f(x_min), 0, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We start with a base class for the gradient descent algorithm. Recall that gradient descent can be used to find a local minimum of a function $f(x)$\n",
    "\n",
    "The algorithm is the following:\n",
    "\n",
    "* Choose initial point $x_0$\n",
    "* Make steps in the direction of the anti-gradient\n",
    "$$\n",
    "x_{t+1} = x_t - \\nu \\nabla f(x_t), \\\\\n",
    "$$\n",
    "* Repeat until stopping criterion is satisfied. Possible stopping criterions are:\n",
    "    - $\\|x_{t+1} - x_t\\| < \\varepsilon$\n",
    "    - $\\|\\nabla f(x_{t+1})\\| < \\varepsilon$\n",
    "    \n",
    "**Task 1.2** [3 pts] <a class=\"anchor\" id=\"task2\"></a>\n",
    "\n",
    "Implement the following methods in the class below:\n",
    "* `step`, which makes one step of the gradient descent algorithm \n",
    "* `stoppin_criterion`, we will use the first option, i.e. $\\|x_{t+1} - x_t\\| < \\varepsilon$\n",
    "* `find_min`, which finds local minimum of a functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5283b2c3c1849d3dd679fe1909b39e6f",
     "grade": false,
     "grade_id": "cell-bba776dd1af9a6b3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BaseGD:\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm with constant learning rate.\n",
    "    On each iteration algorithm makes a step in the direction of the anti-gradeint.\n",
    "    If stopping criterion is statified\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fn : callable\n",
    "        Evaluates the target function\n",
    "    grad_fn : callable\n",
    "        Evaluates gradient of the target function\n",
    "    lr: float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fn, grad_fn, lr=1e-2):\n",
    "        self.fn = fn\n",
    "        self.grad_fn = grad_fn\n",
    "        self.lr = lr\n",
    "        self.hist = []\n",
    "        \n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        Returns learning rate for a given iteration. In the base version the lr is constant\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        out: float\n",
    "            Learning rate for a current iteration\n",
    "        \"\"\"\n",
    "        return self.lr\n",
    "        \n",
    "    def step(self, x):\n",
    "        \"\"\"\n",
    "        Makes one step of the GD: evaluates gradient in the current point \n",
    "        and makes step in the opposite direction with learning rate defined in the method `get_lr`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array_like\n",
    "            Current point\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        out: ndarray\n",
    "            Next point\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "    def stoppin_criterion(self, x_old, x_new, eps):\n",
    "        \"\"\"\n",
    "        Test if the stopping criterion is statisfied \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_old: array_like\n",
    "            Current point\n",
    "        x_new: array_like\n",
    "            Next point\n",
    "        eps: float\n",
    "            Tolerance level\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        out: bool\n",
    "            If True, then no more steps should be done\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "    def find_min(self, x_0, max_iter=int(1e3), eps=1e-4):\n",
    "        \"\"\"\n",
    "        Performs gradient descent starting from x_0. \n",
    "        On each iteration it \n",
    "         - makes step using `self.step`\n",
    "         - saves new point to the history\n",
    "         - check stopping criterion. If it is statisfied, stop and return last point from the history\n",
    "        \n",
    "        Return\n",
    "        ----------\n",
    "        out: tuple\n",
    "            Local minimum found by GD and the value of target function\n",
    "        \"\"\"\n",
    "        self.hist = [x_0]\n",
    "        for _ in range(max_iter):\n",
    "            # your code here\n",
    "            \n",
    "        return self.hist[-1], self.fn(self.hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "172046ae712cfc5684888d2c686d9f80",
     "grade": true,
     "grade_id": "cell-295a8d651a470dee",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST step()\n",
    "\n",
    "# create test functions and initi GD\n",
    "test_f = lambda x: x ** 4\n",
    "test_grad_f = lambda x: 4 * x ** 3\n",
    "my_gd = BaseGD(test_f, test_grad_f, lr=0.1)\n",
    "\n",
    "x_step_l = my_gd.step(-1.)\n",
    "assert np.allclose(x_step_l, -0.6), 'Check your `step` method'\n",
    "\n",
    "\n",
    "prenabla = lambda x: x ** 3\n",
    "nabla = lambda x: 3 * x ** 2\n",
    "my_gd = BaseGD(prenabla, nabla, lr=0.1)\n",
    "x_step_l = my_gd.step(-1.)\n",
    "x_step_r = my_gd.step(1.)\n",
    "assert np.allclose(x_step_l, -1.3),  'Check your `step` method'\n",
    "assert np.allclose(x_step_r, 0.7),  'Check your `step` method'\n",
    "assert(np.allclose(x_step_l - x_step_r, -2., atol=1e-3)),  'Check your `step` method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "839b349c7e6251122fd0dd3861fbd5d1",
     "grade": true,
     "grade_id": "cell-d1cce9acebc22aef",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST stoppin_criterion()\n",
    "test_f = lambda x: x ** 4\n",
    "test_grad_f = lambda x: 4 * x ** 3\n",
    "\n",
    "my_gd = BaseGD(test_f, test_grad_f, lr=0.1)\n",
    "eps = 1e-3\n",
    "\n",
    "left = my_gd.stoppin_criterion(-1., 1., eps)\n",
    "assert ~left, 'Check your `stoppin_criterion` method'\n",
    "\n",
    "\n",
    "test_f = lambda x: x ** 4\n",
    "test_grad_f = lambda x: 4 * x ** 3\n",
    "\n",
    "my_gd = BaseGD(prenabla, nabla, lr=0.1)\n",
    "eps = 1e-3\n",
    "left = my_gd.stoppin_criterion(-1. + eps, 1., eps)\n",
    "assert ~left, 'Check your `stoppin_criterion` method'\n",
    "\n",
    "right = my_gd.stoppin_criterion(1., -1+eps, eps)\n",
    "assert ~right, 'Check your `stoppin_criterion` method'\n",
    "assert(left == right), 'Check your `stoppin_criterion` method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dec7f98dd6d61266821b0bc43c2324d7",
     "grade": true,
     "grade_id": "cell-88c78dc6de704aff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST find_min()\n",
    "test_f = lambda x: x ** 2\n",
    "test_grad_f = lambda x: 2 * x\n",
    "my_gd = BaseGD(test_f, test_grad_f, lr=0.1)\n",
    "x_min, _ = my_gd.find_min(x_0=.5)\n",
    "assert(np.allclose(x_min, 0.0, atol=1e-3)), 'GD does not coverge to the minimum, check your `find_min` method'\n",
    "\n",
    "\n",
    "prenabla = lambda x: (x - 5.) ** 2\n",
    "nabla = lambda x: 2 * (x - 5.)\n",
    "my_gd = BaseGD(prenabla, nabla, lr=0.1)\n",
    "x_min, _ = my_gd.find_min(x_0=.5)\n",
    "assert(np.allclose(x_min, 5.0, atol=1e-3)), 'GD does not coverge to the minimum, check your `find_min` method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de99cb5ba9dbb78f5843a2ed814a1b39",
     "grade": false,
     "grade_id": "cell-e6c15494a0ca33e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(fn, history):\n",
    "    N_steps = len(history)\n",
    "    assert N_steps > 1\n",
    "    \n",
    "    # plot all the points\n",
    "    plt.scatter(history, fn(np.array(history)), \n",
    "                c='red', s=40, lw=0, alpha=0.5);\n",
    "\n",
    "    # draw the arrows\n",
    "    for j in range(1, N_steps):\n",
    "        plt.annotate('', xy=[history[j], fn(history[j])],  xytext=[history[j-1], fn(history[j-1])], \n",
    "                     arrowprops={'arrowstyle': '-', 'color': 'r', 'lw': 1, 'alpha':0.8},\n",
    "                     va='center', ha='center')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f211ed38729a01c47dc7e9ded29b76b",
     "grade": false,
     "grade_id": "cell-9da226149e6ce428",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 1.3** [1 pt] <a class=\"anchor\" id=\"task3\"></a>\n",
    "\n",
    "Now it is time to use our algorithm. Assume that the starting point is fixed $x_0 = 9$. Your task is to tune hyperparameters of the method, so that it coverges to a **global** minimum (or to the point wich is close enough to the global minimum). Value of the target function in the obtained point should smaller that `-12.5`. You can tune different parameters:\n",
    "- learning rate (`lr`)\n",
    "- maximal number of iterations (`max_iter`)\n",
    "- tolerance of the stopping criterion (`eps`)\n",
    "\n",
    "Please, fill in the function `find_global_opt`, where you should create instance of the BaseGD class, call method `find_min` and returns instance of `BaseGD`. \n",
    "\n",
    "Below is an example of how the solution may look like:\n",
    "```\n",
    "def find_global_opt():\n",
    "    gd = BaseGD(f, grad_f)\n",
    "    x_min, f_min = gd.find_min(x_0)\n",
    "    return gd\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "045b527abb7eb98b8e4b7baf1100626d",
     "grade": false,
     "grade_id": "cell-46dfec3912a9a569",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_0 = 9 # !! do not change starting point\n",
    "\n",
    " \n",
    "def find_global_opt():\n",
    "    \"\"\"\n",
    "    Defines optimal hyperparameter for, runs gd and returns the instance of the correcponding class\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b317409c4782f091cf79cc390099659a",
     "grade": false,
     "grade_id": "cell-10b568cf15313735",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below you can visually test that the obtained solution is close to global minima (it is only for you to check youself visually, the plot will not be checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7e5a14b4a93b31170283672eb0c0544",
     "grade": false,
     "grade_id": "cell-8c4b17b212e47bb2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the initial function\n",
    "plot_function(f)\n",
    "\n",
    "# get instance of the gd class an plot its history\n",
    "gd = find_global_opt()\n",
    "plot_history(f, gd.hist)\n",
    "\n",
    "# extract last point from the history and plot\n",
    "x_min = gd.hist[-1]\n",
    "f_min = gd.fn(x_min)\n",
    "plt.scatter(x_min, f_min, s=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3323ce30b168f10d7088c01cdd098522",
     "grade": true,
     "grade_id": "cell-953bdcf5dd6d5f35",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST find_global_opt\n",
    "gd = find_global_opt()\n",
    "print('x_min = {:.2f}'.format(gd.hist[-1]))\n",
    "print('f(x_min) = {:.2f}'.format(gd.fn(gd.hist[-1])))\n",
    "\n",
    "\n",
    "gd = find_global_opt()\n",
    "x_min = gd.hist[-1]\n",
    "\n",
    "# test that fun value is small enough\n",
    "assert gd.fn(x_min) <= -12.5, 'Value of the function is larger than -12.5'\n",
    "\n",
    "# Test that history is proper \n",
    "assert gd.hist[0] == 9, 'Starting point is not 9'\n",
    "assert gd.step(gd.hist[-2]) == gd.hist[-1], 'History is not proper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task 1.4** [1 point] <a class=\"anchor\" id=\"task4\"></a>\n",
    "\n",
    "You've probably noticed that it is not easy to select proper learning rate for all the iterations. Let us now make the learning rate dependent on the step number.  \n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\nu_t \\nabla f(x_t), \\\\\n",
    "\\nu_t = \\frac{\\nu_0}{t}\n",
    "$$\n",
    "\n",
    "Where $\\nu_0$ is the initial value and it is reduced with a constant speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4161b3e75472da3a9cf9cbd488f8a9c2",
     "grade": false,
     "grade_id": "cell-f6db94df18ddb0ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LRStepGD(BaseGD):\n",
    "    def __init__(self, fn, grad_fn, start_lr):\n",
    "        super().__init__(fn, grad_fn, start_lr)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        Returns learning rate for a given iteration using formula $\\nu_t = \\nu_0* 1/t\n",
    "        Note that number of the current iteration can be extracted from the list with the history\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        out: float\n",
    "            Learning rate for a current iteration\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2301b2d459a6d8a3e338b987ce11f06",
     "grade": true,
     "grade_id": "cell-0c362c89da300873",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST get_lr\n",
    "prenabla = lambda: x ** 2\n",
    "nabla = lambda: 2 * x\n",
    "lr0 = 1e-1\n",
    "test_gd = LRStepGD(prenabla, nabla, lr0)\n",
    "assert(test_gd.get_lr() == lr0), 'Method `get_lr` should return initial learning rate on the first iteration'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now try different starting points and initial learing rate. Check that it is easier to reach global optimum, because you can set large enough initial learing rate to jump over local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = -7 # Try changing inital point\n",
    "start_lr = 0.3 # and initial learing learning rate\n",
    "\n",
    "gd = LRStepGD(f, grad_f, start_lr)\n",
    "x_min, f_min = gd.find_min(x_0)\n",
    "\n",
    "# draw plots\n",
    "plot_function(f)\n",
    "plot_history(f, gd.hist)\n",
    "plt.scatter(x_min, f_min, s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dc7bec6d5c923465b028bc68702d470",
     "grade": false,
     "grade_id": "cell-a5cb5f37ad4de405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Problem 2. Weighted Linear Regression <a class=\"anchor\" id=\"part2\"></a>\n",
    "---\n",
    "\n",
    "### Linear Regression Recap\n",
    "\n",
    "In simple linear regression we have assumed that all the observations are of the same 'importance' to the model. In practice it is not always the case. Due to different reasons, it may happen that some observations are more valuable for us than others. In this problem we will see, how we can take into account such infomation.\n",
    "\n",
    "Let us start with recapping the simple linear regression.\n",
    "\n",
    "* **Model** with k features:\n",
    "$$\n",
    "a(x) = w_0 + w_1x^1 + \\dots w_kx^k = \\langle w, x \\rangle,\\\\\n",
    "x = (1, x^1, \\dots, x^k)\n",
    "$$\n",
    "\n",
    "* **Dataset:** \n",
    "$$\\text{design matrix } X \\in \\mathbb{R}^{N \\times k+1},\\\\\n",
    "\\text{target values }y \\in \\mathbb{R}^{N}$$\n",
    "\n",
    "* **MSE Loss**:\n",
    "$$\n",
    "L = \\tfrac{1}{N}\\| y - Xw\\|^2_2\n",
    "$$\n",
    "\n",
    "We obtained the matrix form of the MSE loss inthe following manner:\n",
    "\n",
    "\n",
    "$$\n",
    "L(a, X) = \\frac{1}{N}\\sum_{i=1}^N (y_i -  a(x_i))^2 =  \\frac{1}{N}\\sum_{i=1}^N (y_i -  \\langle w, x_i \\rangle)^2 =  \\frac{1}{N}(y - Xw)^T(y - Xw) =  \\frac{1}{N} \\| y - Xw\\|^2_2\n",
    "$$\n",
    "\n",
    "\n",
    "From the lecture you know that in this case optimal parameters (that minimize the loss function) can be written in a closed-form:\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\left(X^T X\\right)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weighted Linear Regression\n",
    "Assume now, that some observations in our dataset are more \"important\" that others. E.g. we know that for some points the measurements are less accurate and want to reduce the weight of such observation. Another possible reason: we assume more recent observations to be more relevant and want to account for that in our loss function. \n",
    "\n",
    "* The **model**  (exactly the same):\n",
    "$$\n",
    "a(x) = w_0 + w_1x^1 + \\dots w_kx^k\n",
    "$$\n",
    "\n",
    "* The **dataset**: \n",
    "$$\\text{design matrix } X \\in \\mathbb{R}^{N \\times k+1},\\\\\n",
    "\\text{target values }y \\in \\mathbb{R}^{N}$$\n",
    "\n",
    "In addition, we have vector of weights, which reflects the importance of each observation:\n",
    "$$v = (v_1, \\dots, v_N)$$ \n",
    "\n",
    "* **MSE loss**:\n",
    "\n",
    "We will change loss function, so that it includes the weights:\n",
    "$$\n",
    "L(a, X) = \\frac{1}{N}\\sum_{i=1}^N v_i(y_i -  a(x_i))^2\n",
    "$$\n",
    "\n",
    "In the matrix form, it will looks like this:\n",
    "$$\n",
    "L(a, X) =  \\tfrac{1}{N}\\| V^{1/2}(y - Xw)\\|^2_2\n",
    "$$\n",
    "\n",
    "Where $V$ is matrix with weight $v$ on the diagonal and zeros elsewhere:\n",
    "\n",
    "$$\n",
    "V = \\begin{pmatrix}\n",
    "v_1 & 0 & \\cdots & 0\\\\\n",
    "0  & v_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots& \\vdots \\\\\n",
    "0 & 0 & \\cdots & v_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In this task we will train weighted linear regression using both closed form solution and gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task 2.1** [1 pt]  <a class=\"anchor\" id=\"task2_1\"></a>\n",
    "\n",
    "Calculate gradient of the weighted MSE loss with respect to parameters for the model\n",
    "\n",
    "$$\n",
    "\\nabla_w L =  \\nabla_w \\tfrac{1}{N}\\| V^{1/2}(y - Xw)\\|^2 = ?\n",
    "$$\n",
    "\n",
    "**Hints:** You can use formulas from the lecture. \n",
    "\n",
    "Given vector $x \\in \\mathbb{R}^n$ and matrix $A \\in \\mathbb{R}^{k \\times n}$\n",
    "$$\n",
    "\\nabla_x \\| x\\|^2_2 = 2x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_x Ax = A^T\n",
    "$$\n",
    "\n",
    "Using the formula frm the gradient, implement the function `weighted_mse_grad`, which calculates gradient for any given vector $w$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d028caf3bd2938efb380a579a8a8344",
     "grade": false,
     "grade_id": "cell-0941c99d2cffad89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_mse_grad(w, X, y, V):\n",
    "    \"\"\"\n",
    "    Calculate gradient of the weight MSE loss in the given point w\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w: array_like\n",
    "        Current point, in which we want to evaluate gradient (vector with k+1 elements)\n",
    "    X: array_like\n",
    "        Design matrix with N rows and k+1 columns\n",
    "    y: array_like\n",
    "        Vector with N observations of the target variable\n",
    "    V: array_like\n",
    "        Diagonal matrix N x N, with the weights\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    out: ndarray\n",
    "        vector of gradient, k+1\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6feabc95062710511fd9a998e0ec3d3f",
     "grade": true,
     "grade_id": "cell-25e2882c43cf7177",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST weighted_mse_grad()\n",
    "w = np.zeros(5)\n",
    "X = np.random.randn(10, 5)\n",
    "y = np.ones(10)\n",
    "V = np.diag(np.ones(10))\n",
    "\n",
    "L_grad = weighted_mse_grad(w, X, y, V)\n",
    "assert(L_grad.shape == (5,))\n",
    "\n",
    "\n",
    "w = np.zeros(5)\n",
    "X = np.eye(5)\n",
    "y = np.ones(5)\n",
    "V = np.diag(np.ones(5))\n",
    "L_grad = weighted_mse_grad(w, X, y, V)\n",
    "assert(L_grad.shape == (5,)), 'The shape of the output on the test case is wrong'\n",
    "assert(np.allclose(5 * L_grad,- y * 2)), 'The values of the output on the test case is wrong'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find analytical solution we need to solve\n",
    "$$\n",
    "\\nabla_w L = 0\n",
    "$$\n",
    "---\n",
    "**Task 2.2** [1 pt] <a class=\"anchor\" id=\"task2_2\"></a>\n",
    "Write a function `weighted_lr`, which calculates optimal parameters for the weighted linear regression\n",
    "\n",
    "**Hint**. Module `linalg` from `scipy` package has many usefull operations, including matrix inversion. It is highly likely that you will need it to solve this task. For example `sp.linalg.inv(A)` will return inverse of the matrix `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee15cb1a44d6bf597970283b70fbf97",
     "grade": false,
     "grade_id": "cell-82dd230f8b3da459",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_lr(X, y, V):\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "929e6374eb0b0661cea1d19d6f29f1bb",
     "grade": true,
     "grade_id": "cell-66547b17a65bacb9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST weighted_lr()\n",
    "X = np.random.randn(10, 5)\n",
    "true_beta = np.random.randn(5)\n",
    "y = X@true_beta\n",
    "V = np.diag(np.ones(10))\n",
    "\n",
    "assert np.allclose(weighted_lr(X, y, V), true_beta), 'Answer is wrong. Check your `weighted_lr` function'\n",
    "\n",
    "\n",
    "X = np.random.randn(10, 5)\n",
    "true_beta = np.random.randn(5)\n",
    "y = X @ true_beta\n",
    "V = 2*np.diag(np.ones(10))\n",
    "assert np.allclose(weighted_lr(X, y, V), true_beta), 'Answer is wrong. Check your `weighted_lr` function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test how the weigted linear regression work. Below we define true linear dependecy between $x$ and $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b595c4f4a9fcfa70170ddeadd559df94",
     "grade": false,
     "grade_id": "cell-6a2431ece9d192f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def true_func(x):\n",
    "    return 1. + 2. * x \n",
    "\n",
    "def predict(X, w):\n",
    "    return  X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now, that half of the points were measured with the error. E.g. due to the fact that we've used less accurate device to collect them. Below we artificially generate such dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0d9e08c301e70bdd8f7d809969e2b7a",
     "grade": false,
     "grade_id": "cell-8b7146663831ee82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "N = 15\n",
    "np.random.seed(1)\n",
    "x_good = np.random.uniform(-4, 4, N) \n",
    "y_good = true_func(x_good) + np.random.randn(N)\n",
    "\n",
    "x_bad = np.random.uniform(-4, 4, N) \n",
    "y_bad = true_func(x_bad) + np.abs(np.random.randn(N)*4)\n",
    "\n",
    "plt.scatter(x_good, y_good, label='Good points')\n",
    "plt.scatter(x_bad, y_bad, label='high noise_points')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "x_full = np.concatenate([x_good, x_bad])\n",
    "y_full = np.concatenate([y_good, y_bad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let use define two weight metrices. One in the assumptione that all the points are equal. The latter will take into account the fact that second half of the objects in the dataset are more noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7fcaf1a0487a9a5a690cf4e6091852c",
     "grade": false,
     "grade_id": "cell-b5e78e5c0e485057",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.stack([np.ones(2*N), x_full], 1)\n",
    "\n",
    "v_none = np.diag(np.ones(2*N))\n",
    "print(v_none[:3, :3])\n",
    "\n",
    "v_smart = np.ones(2*N)\n",
    "v_smart[:N] *= 4\n",
    "v_smart = np.diag(v_smart)\n",
    "print(v_smart[:3, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4786a2c3f5f23c8cf598fd7c29a5c1d7",
     "grade": false,
     "grade_id": "cell-803f4ac72e2db3d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "**Task 2.3** [1 pt] <a class=\"anchor\" id=\"task2_3\"></a>\n",
    "Using weigth matrices `v_none` and `v_smart`, find optimal parameters of the weighted linear regressions. Call them `w_none` and `w_smart` correspondingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a282433933e62a8d687b60f55429cb4",
     "grade": false,
     "grade_id": "cell-83716c19dbc8c7ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c46a5e012bf9d81a838252ac0ce4e23a",
     "grade": true,
     "grade_id": "cell-adc22f0a29e721c8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(w_none)\n",
    "print(w_smart)\n",
    "\n",
    "assert np.allclose(w_none[0], 2.4865, rtol=1e-3), 'First value of the `w_none` is wrong'\n",
    "assert np.allclose(w_none[1], 1.9015, rtol=1e-3),  'Second value of the `w_none` is wrong'\n",
    "assert np.allclose(w_smart[0], 1.7657, rtol=1e-3),  'First value of the `w_smart` is wrong'\n",
    "assert np.allclose(w_smart[1],  2.0082, rtol=1e-3),  'Second value of the `w_smart` is wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.arange(-4, 4, 0.1)\n",
    "lr_predict = predict(np.stack([np.ones(x_grid.shape[0]), x_grid], 1), w_none)\n",
    "wlr_predict = predict(np.stack([np.ones(x_grid.shape[0]), x_grid], 1), w_smart)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax[0].plot(x_grid, true_func(x_grid), label='True function')\n",
    "ax[0].plot(x_grid, lr_predict, label='Simple LR', c='Orange')\n",
    "ax[0].plot(x_grid, wlr_predict, label='Weighted LR', c='Green')\n",
    "ax[0].set_title('Predicted line')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "\n",
    "ax[1].plot(x_grid, (true_func(x_grid) - lr_predict) ** 2, label='Simple LR', c='Orange')\n",
    "ax[1].plot(x_grid, (true_func(x_grid) - wlr_predict) ** 2, label='Weighted LR', c='Green')\n",
    "ax[1].set_title('Model error')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('Error')\n",
    "\n",
    "for i in ax:\n",
    "    i.legend()\n",
    "    i.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
